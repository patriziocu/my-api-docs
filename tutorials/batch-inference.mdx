---
title: "Run Batch Inference"
description: "How to process large datasets against our API"
---

## Overview

When processing thousands of records, it is best to send them in **batches** rather than one by one. This script demonstrates how to:

1.  Generate mock data.
2.  Chunk the data into batches of 50.
3.  Send requests to the API.
4.  Save the results to a JSON file.

### The Script

```python title="batch_inference.py"
import time
import json
import random

# Configuration
API_URL = "[https://api.example.com/v1/predict](https://api.example.com/v1/predict)"
BATCH_SIZE = 50
TOTAL_RECORDS = 500

def generate_mock_data(n):
    """Generates a list of dummy input data."""
    data = []
    for i in range(n):
        record = {
            "id": f"input_{i}",
            "features": [random.random() for _ in range(5)]
        }
        data.append(record)
    return data

def process_batches(data, batch_size):
    """Iterates through data in chunks."""
    results = []
    
    # Calculate total batches needed
    total_batches = len(data) // batch_size + (1 if len(data) % batch_size > 0 else 0)
    
    print(f"Starting inference on {len(data)} records ({total_batches} batches)...")

    for i in range(0, len(data), batch_size):
        batch = data[i : i + batch_size]
        batch_num = (i // batch_size) + 1
        
        # Simulate API Request
        # In real life: response = requests.post(API_URL, json={"inputs": batch})
        print(f"Processing Batch {batch_num}/{total_batches} ({len(batch)} records)...")
        
        # Mocking network delay
        time.sleep(0.5) 
        
        # Mocking API response
        batch_predictions = [
            {"id": item["id"], "prediction": random.choice([0, 1]), "confidence": round(random.random(), 2)}
            for item in batch
        ]
        
        results.extend(batch_predictions)

    return results

if __name__ == "__main__":
    # 1. Load Data
    my_data = generate_mock_data(TOTAL_RECORDS)
    
    # 2. Run Inference
    all_predictions = process_batches(my_data, BATCH_SIZE)
    
    # 3. Save Results
    output_filename = "inference_results.json"
    with open(output_filename, "w") as f:
        json.dump(all_predictions, f, indent=2)
        
    print(f"âœ… Success! Saved {len(all_predictions)} predictions to {output_filename}")
